{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by specifying:\n",
    "\n",
    "    The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "\n",
    "    The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the boto regexp with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name AmazonSageMaker-ExecutionRole-20210526T221366 to get Role path.\n",
      "Assuming role was created in SageMaker AWS console, as the name contains `AmazonSageMaker-ExecutionRole`. Defaulting to Role ARN with service-role in path. If this Role ARN is incorrect, please add IAM read permissions to your role or supply the Role Arn directly.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-linear-mnist\"\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion\n",
    "\n",
    "Next, we read the dataset from an online URL into memory, for preprocessing prior to training. This processing could be done in situ by Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn’t onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 779 ms, sys: 272 ms, total: 1.05 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle, gzip, numpy, urllib.request, json\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open(\"mnist.pkl.gz\", \"rb\") as f:\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (2, 10)\n",
    "\n",
    "\n",
    "def show_digit(img, caption=\"\", subplot=None):\n",
    "    if subplot == None:\n",
    "        _, (subplot) = plt.subplots(1, 1)\n",
    "    imgr = img.reshape((28, 28))\n",
    "    subplot.axis(\"off\")\n",
    "    subplot.imshow(imgr, cmap=\"gray\")\n",
    "    plt.title(caption)\n",
    "\n",
    "\n",
    "show_digit(train_set[0][30], \"This is a {}\".format(train_set[1][30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "vectors = np.array([t.tolist() for t in train_set[0]]).astype(\"float32\")\n",
    "labels = np.where(np.array([t.tolist() for t in train_set[1]]) == 0, 1, 0).astype(\"float32\")\n",
    "\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, vectors, labels)\n",
    "buf.seek(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "key = \"recordio-pb-data\"\n",
    "boto3.resource(\"s3\").Bucket(bucket).Object(os.path.join(prefix, \"train\", key)).upload_fileobj(buf)\n",
    "s3_train_data = \"s3://{}/{}/train/{}\".format(bucket, prefix, key)\n",
    "print(\"uploaded training data location: {}\".format(s3_train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, \"linear-learner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the linear model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. Since this data is relatively small, it isn’t meant to show off the performance of the Linear Learner training algorithm, although we have tested it on multi-terabyte datasets.\n",
    "\n",
    "Again, we’ll use the Amazon SageMaker Python SDK to kick off training, and monitor status until it is completed. In this example that takes between 7 and 11 minutes. Despite the dataset being small, provisioning hardware and loading the algorithm container take time upfront.\n",
    "\n",
    "First, let’s specify our containers. Since we want this notebook to run in all 4 of Amazon SageMaker’s regions, we’ll create a small lookup. More details on algorithm containers can be found in AWS documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "linear = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "linear.set_hyperparameters(feature_dim=784, predictor_type=\"binary_classifier\", mini_batch_size=200)\n",
    "\n",
    "linear.fit({\"train\": s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up hosting for the model\n",
    "\n",
    "Now that we’ve trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_predictor = linear.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "linear_predictor.content_type = \"text/csv\"\n",
    "linear_predictor.serializer = csv_serializer\n",
    "linear_predictor.deserializer = json_deserializer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = linear_predictor.predict(train_set[0][30:31])\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "predictions = []\n",
    "for array in np.array_split(test_set[0], 100):\n",
    "    result = linear_predictor.predict(array)\n",
    "    predictions += [r[\"predicted_label\"] for r in result[\"predictions\"]]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c89008c78acc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m pd.crosstab(\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrownames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"actuals\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.crosstab(\n",
    "    np.where(test_set[1] == 0, 1, 0), predictions, rownames=[\"actuals\"], colnames=[\"predictions\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "If you’re ready to be done with this notebook, please run the delete_endpoint line in the cell below. This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sagemaker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f4940a87fb1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sagemaker' is not defined"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(linear_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
